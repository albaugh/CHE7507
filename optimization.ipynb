{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOP1AmX035Fri7rKEQt2G0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albaugh/CHE7507/blob/main/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Albaugh.  Wayne State.  CHE 5995/7507.  Lecture 9.  Winter 2026."
      ],
      "metadata": {
        "id": "T663XvpEE_ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of optimization is to find the minimum or maximum of a function.  This is particularly useful in machine learning because we want to train our models to minimize their error.  In this class we'll be focused on gradient-based optimization methods.  If I can take the gradient (derivative) of a function, then its minimum will be in the opposite direction of the gradient- it will be downhill.  \n",
        "\n",
        "We will first see how to automatically differentiate a function using <code>jax</code> and then we'll use these gradients to optimize functions."
      ],
      "metadata": {
        "id": "8VkQZRvsyWZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation"
      ],
      "metadata": {
        "id": "lmvcjTf_zBmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <code>jax</code> library is useful for automatic differentiation.  Here we'll first define a function and then use <code>jax</code> synthax and functions to get its derivative without having to do any handwritten math ourselves.  Here we'll test with the function $y = (x+a)(x+b)(x+c)(x+d)$."
      ],
      "metadata": {
        "id": "eJgU4N1KzKsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "8mt76PMb1vAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function\n",
        "def f(x):\n",
        "  a = -2.0\n",
        "  b = 0.0\n",
        "  c = 1.0\n",
        "  d = -3.0\n",
        "\n",
        "  y = (x+a)*(x+b)*(x+c)*(x+d)\n",
        "  return y\n",
        "\n",
        "#use jax to define the gradient of the function\n",
        "df_dx = jax.grad(f)\n",
        "\n",
        "#let's see how it works\n",
        "x = np.linspace(-1.5, 3.0, 10000)\n",
        "y = []\n",
        "dydx = []\n",
        "\n",
        "#let's also see how long this calculation takes\n",
        "start_time = time.time()\n",
        "for i in x:\n",
        "  #calculate the function and its derivatives\n",
        "  y.append(f(i))\n",
        "  dydx.append(df_dx(i))\n",
        "\n",
        "end_time = time.time()\n",
        "print('Calculation time: ', end_time-start_time, ' seconds')\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(x, y, color='orange', label='$f(x)$')\n",
        "ax.plot(x, dydx, color='green', label='$\\\\frac{df}{dx}$')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gMtEUaTu4TSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was pretty slow.  The reason it was slow is that for every single calculation, <code>jax</code> is recomputing what the derivative function needs to be.  Using <code>jit</code> tells the program to only compile the functions once at the beginning and it will then reuse them, which makes everything much faster.  It stands for 'just-in-time' compilation."
      ],
      "metadata": {
        "id": "A6LlixCc6KJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define function\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  a = -2.0\n",
        "  b = 0.0\n",
        "  c = 1.0\n",
        "  d = -3.0\n",
        "\n",
        "  y = (x+a)*(x+b)*(x+c)*(x+d)\n",
        "  return y\n",
        "\n",
        "#use jax to define the gradient of the function\n",
        "df_dx = jax.jit(jax.grad(f))\n",
        "\n",
        "#let's see how it works\n",
        "x = np.linspace(-1.5, 3.0, 10000)\n",
        "y = []\n",
        "dydx = []\n",
        "\n",
        "#let's see how long this calculation takes\n",
        "start_time = time.time()\n",
        "\n",
        "for i in x:\n",
        "  y.append(f(i))\n",
        "  dydx.append(df_dx(i))\n",
        "\n",
        "end_time = time.time()\n",
        "print('Calculation time: ', end_time-start_time, ' seconds')\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(x, y, color='orange', label='$f(x)$')\n",
        "ax.plot(x, dydx, color='green', label='$\\\\frac{df}{dx}$')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CyFo_5-X6dEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also vectorize the computation with <code>numpy</code> arrays, but they have to be special <code>jax</code> enabled <code>numpy</code> arrays.  This is how it works."
      ],
      "metadata": {
        "id": "jvTwzmNc4zxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zczxBgDYyRMd"
      },
      "outputs": [],
      "source": [
        "#define function\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  a = -2.0\n",
        "  b = 0.0\n",
        "  c = 1.0\n",
        "  d = -3.0\n",
        "\n",
        "  y = (x+a)*(x+b)*(x+c)*(x+d)\n",
        "  return y\n",
        "\n",
        "#use jax to define the gradient of the function\n",
        "df_dx = jax.jit(jax.grad(f))\n",
        "\n",
        "#this time we create an array with jnp- jax numpy\n",
        "x = jnp.linspace(-1.5, 3.0, 10000)\n",
        "\n",
        "#let's see how long this calculation takes\n",
        "start_time = time.time()\n",
        "\n",
        "#we also vectorize the gradient and function computation\n",
        "dydx = jax.vmap(df_dx)(x)\n",
        "y = jax.vmap(f)(x)\n",
        "\n",
        "end_time = time.time()\n",
        "print('Calculation time: ', end_time-start_time, ' seconds')\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(x, y, color='orange', label='$f(x)$')\n",
        "ax.plot(x, dydx, color='green', label='$\\\\frac{df}{dx}$')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use <code>jax</code> to get the derivative of functions with multiple inputs.  For inputs $x_1$, $x_2$,...,$x_n$, <code>jax</code> can return the gradient $\\nabla f = [\\partial f/\\partial x_1, \\partial f/\\partial x_2,...,\\partial f/\\partial x_n, ]^{T}$.  Here we'll test with the function $y = (x_1+a)(x_2+b)(x_3+c)(x_1+d)$."
      ],
      "metadata": {
        "id": "IxWjxnj69RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define function\n",
        "@jax.jit\n",
        "def f(x1, x2, x3):\n",
        "  a = -2.0\n",
        "  b = 0.0\n",
        "  c = 1.0\n",
        "  d = -3.0\n",
        "\n",
        "  y = (x1 + a)*(x2 + b)*(x3 + c)*(x1 + d)\n",
        "  return y\n",
        "\n",
        "#use jax to define the gradient of the function\n",
        "df = jax.jit(jax.grad(f, argnums=(0, 1, 2))) #this means differentiate with respect to the first 3 arguments of the function\n",
        "\n",
        "#this time we create an array with jnp- jax numpy\n",
        "x1 = jnp.linspace(-1.5, 3.0, 10000)\n",
        "x2 = jnp.linspace(-1.5, 3.0, 10000)\n",
        "x3 = jnp.linspace(-1.5, 3.0, 10000)\n",
        "\n",
        "#let's see how long this calculation takes\n",
        "start_time = time.time()\n",
        "\n",
        "#we also vectorize the gradient and function computation\n",
        "gradient = jax.vmap(df)(x1, x2, x3)\n",
        "y = jax.vmap(f)(x1, x2, x3)\n",
        "\n",
        "end_time = time.time()\n",
        "print('Calculation time: ', end_time-start_time, ' seconds')\n"
      ],
      "metadata": {
        "id": "okHa5VwS9Qst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent"
      ],
      "metadata": {
        "id": "ms-XRXY5-97N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've figured out how to get gradients, now we can do gradient descent.  With this we iteratively take steps in the direction of the negative gradient of a function to find its minimum.  Let's start by finding the minimum value of a parabola $y = (x-1)^2 + 2$."
      ],
      "metadata": {
        "id": "rCWBI2hL_AUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  return (x-1)**2 + 2.0\n",
        "\n",
        "#derivative with jax\n",
        "df = jax.jit(jax.grad(f))\n",
        "\n",
        "#make a function for gradient descent\n",
        "def gradient_descent(x, lr):\n",
        "  #set up an array to record what our estimates for the minimum x-value is\n",
        "  record = []\n",
        "  record.append(x)\n",
        "\n",
        "  #iteratively calculate the gradient and use it to update our guess for x\n",
        "  converged = False\n",
        "  eps = 1e-5\n",
        "  iterations = 0\n",
        "  max_iterations = 1000\n",
        "  while not converged:\n",
        "    #calculate the gradient\n",
        "    grad = df(x)\n",
        "\n",
        "    #update x\n",
        "    x = x - lr*grad\n",
        "\n",
        "    #record this value of x\n",
        "    record.append(x)\n",
        "\n",
        "    #update the number of iterations\n",
        "    iterations += 1\n",
        "\n",
        "    #check to see if we've converged\n",
        "    if abs(grad) < eps or iterations > max_iterations:\n",
        "      converged = True\n",
        "\n",
        "  return x, record, iterations\n",
        "\n",
        "#define the learning rate\n",
        "lr = 0.1\n",
        "\n",
        "#start with a guess for where the minimum is\n",
        "x = -3.0\n",
        "\n",
        "#do gradient descent\n",
        "x, rec, iter = gradient_descent(x, lr)\n",
        "\n",
        "\n",
        "#print the value of the location of the minimum and the value of the function at the minimum:\n",
        "print('Number of iterations: ', iter)\n",
        "print('Location of minimum: ', x)\n",
        "print('Value of minimum: ', f(x))\n",
        "\n",
        "#plot the initial guess, final guess, and function\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(rec[0], f(rec[0]), color='r', marker='o', label='initial guess')\n",
        "ax.scatter(rec[-1], f(rec[-1]), color='g', marker='o', label='final guess')\n",
        "x_plot = jnp.linspace(-4.0, 4.0, 1000)\n",
        "y_plot = jax.vmap(f)(x_plot)\n",
        "ax.plot(x_plot, y_plot, color='blue', label='$f(x)$')\n",
        "ax.set_xlabel('$x$',fontsize=20)\n",
        "ax.set_ylabel('$f(x)$',fontsize=20)\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "\n",
        "#let's also see how the value of x changes through the minimization procedure\n",
        "fig2, ax2 = plt.subplots()\n",
        "ax2.plot(np.arange(0,iter+1), rec, color='purple')\n",
        "ax2.set_xlabel('iteration',fontsize=20)\n",
        "ax2.set_ylabel('$x$',fontsize=20)\n",
        "ax2.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8WMzM11x-_vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the effect of learning rate.  We'll see how quickly (or slowly) we approach the solution based on the learning rate."
      ],
      "metadata": {
        "id": "nzEalfwVFzsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#look at some different learning rates\n",
        "learning_rates = [0.9, 0.8, 0.6, 0.5, 0.1, 0.05]\n",
        "colors = ['k','purple','b','g','gold','orange']\n",
        "\n",
        "#set up a plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for lr,c in zip(learning_rates,colors):\n",
        "  #start with a guess for where the minimum is\n",
        "  x = -3.0\n",
        "\n",
        "  #do gradient descent\n",
        "  x, rec, iter = gradient_descent(x, lr)\n",
        "\n",
        "  ax.plot(np.arange(1,iter+2), rec, color=c, label='learning rate = '+str(lr))\n",
        "\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('iteration',fontsize=20)\n",
        "ax.set_ylabel('$x$',fontsize=20)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-QUrg7EwBUYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is usually an optimal learning rate.  If the rate is too low, it takes many iterations as the algorithm takes tiny steps towards the minimum.  If the rate is too high then the algorithm can overshoot the minimum and take many iterations to oscillate towards the minimum.  If the learning rate is very high, the algorithm can become unstable, never reaching the minimum.  Let's look at a 2-D example, too.  Here we'll try to find the minimum of $y = -3 e^{ - \\frac{1}{2} x_1^2 + 2 x_2^2 }$."
      ],
      "metadata": {
        "id": "RZNphsxyIAI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function\n",
        "@jax.jit\n",
        "def f(x1, x2):\n",
        "  return -3.0 * jnp.exp( -(0.5*x1**2 + 2.0*x2**2) )\n",
        "\n",
        "#derivative with jax\n",
        "df = jax.jit(jax.grad(f, argnums=(0, 1)))\n",
        "\n",
        "#make a function for gradient descent\n",
        "def gradient_descent(x1, x2, lr):\n",
        "  #set up an array to record what our estimates for the minimum x-value is\n",
        "  record = []\n",
        "  record.append([x1, x2])\n",
        "\n",
        "  #iteratively calculate the gradient and use it to update our guess for x\n",
        "  converged = False\n",
        "  eps = 1e-5\n",
        "  iterations = 0\n",
        "  max_iterations = 1000\n",
        "  while not converged:\n",
        "    #calculate the gradient\n",
        "    grad = df(x1, x2)\n",
        "\n",
        "    #update x1 and x2\n",
        "    x1 = x1 - lr*grad[0]\n",
        "    x2 = x2 - lr*grad[1]\n",
        "\n",
        "    #record this value of x\n",
        "    record.append([x1, x2])\n",
        "\n",
        "    #update the number of iterations\n",
        "    iterations += 1\n",
        "\n",
        "    #check to see if we've converged\n",
        "    if np.linalg.norm([x1, x2]) < eps or iterations > max_iterations:\n",
        "      converged = True\n",
        "\n",
        "  return x1, x2, record, iterations\n",
        "\n",
        "#define the learning rate\n",
        "lr = 0.1\n",
        "\n",
        "#start with a guess for where the minimum is\n",
        "x1 = -1.9\n",
        "x2 = -1.9\n",
        "\n",
        "#do gradient descent\n",
        "x1, x2, rec, iter = gradient_descent(x1, x2, lr)\n",
        "\n",
        "#print the number of iterations\n",
        "print('Number of iterations: ', iter)\n",
        "\n",
        "# Create a meshgrid for the (x, y) coordinates\n",
        "x_plot = jnp.linspace(-2, 2, 500)\n",
        "y_plot = jnp.linspace(-2, 2, 500)\n",
        "x_grid, y_grid = jnp.meshgrid(x_plot, y_plot)\n",
        "\n",
        "# Evaluate the function over the grid\n",
        "Z = jax.vmap(f)(x_grid, y_grid)\n",
        "\n",
        "# Plot the heatmap using imshow\n",
        "fig,ax = plt.subplots()\n",
        "c = ax.pcolormesh(x_grid, y_grid, Z, cmap='inferno', shading='auto')\n",
        "cbar = fig.colorbar(c, ax=ax)\n",
        "cbar.set_label('$f(x_{1}, x_{2})$',fontsize=14)\n",
        "ax.scatter([row[0] for row in rec], [row[1] for row in rec], color='c', marker='x', label='all guesses')\n",
        "ax.set_xlabel('$x_{1}$',fontsize=18)\n",
        "ax.set_ylabel('$x_{2}$',fontsize=18)\n",
        "ax.scatter(rec[0][0], rec[0][1], color='r', marker='o', label='initial guess')\n",
        "ax.scatter(rec[-1][0], rec[-1][1], color='lime', marker='o', label='final guess')\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0z5BE2lIS3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}