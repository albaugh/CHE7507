{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8B42mDJ6F9ffE7+A4Webj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albaugh/CHE7507/blob/main/polynomial_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Albaugh.  Wayne State.  CHE 5995/7507.  Lecture 6.  Winter 2026."
      ],
      "metadata": {
        "id": "5WgmzKlw2DUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is actually very similar to linear regression.  We simply take power of each of our features $x_i$ up to a chosen order, $1$, $x_i$, $x_i^2$, ..., $x_i^p$.  We then pack out new polynomial features into the data matrix $\\mathbf{X}$.  The polynomial regression parameters are then given by $\\boldsymbol{\\beta}=(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$, just as with linear regression.  Here we'll demonstrate how to do polynomial regression with <code>sklearn</code> and we'll give an example of bias-variance tradeoff."
      ],
      "metadata": {
        "id": "0AVitNT0UV3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "kX3YNWSgOe-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, let's look at a classic non-linear relationship in chemistry- the Sabatier principle.  The catalytic turnover of a heterogeneous reaction has an optimal adsportion energy.  If a reactant adsorbs too weakly too the surface, it will not spend much time at the surface reacting and the rate will be slow.  If a reactant adsorbs too strongly to the surface then it will block the surface from reactants and not release, essentially poisoning the catalyst.  There will be some optimum.  Let's look at some data from the hydrogen evolution reaction 2H$^{+}$ + 2e$^{-}$ $\\to$ H$_2$.  Here $\\log{i_{0}}$ is the logarithm of the exchange current, a measure of the speed of the reaction.  $\\Delta G$ is the free energy of adsorption of H$^{+}$ to the surface.  Each data point represents a different metallic catalyst."
      ],
      "metadata": {
        "id": "quPCBRs8L5Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/albaugh/CHE7507/refs/heads/main/Lecture6/sabatier_train.csv')\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "\n",
        "def plot_data(ax, df):\n",
        "  ax.scatter(df['dG (eV)'], df['Log(i0)'], color='r', marker='x', label='actual data')\n",
        "  ax.set_xlabel('$\\\\Delta G$ (eV)',fontsize=20)\n",
        "  ax.set_ylabel('$\\\\log{i_0}$',fontsize=20)\n",
        "  ax.grid()\n",
        "\n",
        "plot_data(ax, df_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JZdvOiQbVYxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we can see is a classic \"volcano plot\" that exhibits a maximum rate as a function of adsorption energy.  Clearly linear regression is not going to cut it here.  Let's build our data matrix with a linear term in $\\Delta G$ and a square term in $\\Delta G$."
      ],
      "metadata": {
        "id": "xYXERCNOPvIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df_train['dG (eV)'].values\n",
        "X_train = np.column_stack((x, x**2))\n",
        "y_train = df_train['Log(i0)']\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.scatter(X_train[:,0], X_train[:,1], color='g', marker='s')\n",
        "ax.set_xlabel('$\\\\Delta G$',fontsize=20)\n",
        "ax.set_ylabel('$\\\\Delta G^{2}$',fontsize=20)\n",
        "ax.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YLWHjLnoQe7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<code>sklearn</code> also has a <code>polynomial_features</code> function in its <code>preprocessing</code> library, which automates the process of making polynomical features.  This is much more convenient.  We should use <code>include_bias=False</code> because the bias or the intercept will be fit when we do our linear regression."
      ],
      "metadata": {
        "id": "i2UM-f_rRT4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.scatter(X_train[:,0], X_train[:,1], color='g', marker='s')\n",
        "ax.set_xlabel('$\\\\Delta G$',fontsize=20)\n",
        "ax.set_ylabel('$\\\\Delta G^{2}$',fontsize=20)\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "12pqvtCIRlPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our polynomial features, we can fit our model just like we did with linear regression."
      ],
      "metadata": {
        "id": "ndeOcTisSBPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sklearn.linear_model.LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plot_x = np.linspace(np.min(X_train[:,0]), np.max(X_train[:,0]), 100)\n",
        "plot_x_poly = poly_features.transform(plot_x.reshape(-1,1))\n",
        "plot_y = model.predict(plot_x_poly)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train)\n",
        "ax.plot(plot_x, plot_y, color='b', label='2nd-order polynomial fit')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F2lEhXvFSIc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fit our data to higher orders.  In fact, let's fit our data from every possible order- 1$^st$-order through $(N-1)^{th}$-order, where $N$ is the number of training data points that we have.  We'll calculate the MSE for each model and then plot MSE vs. the order of the polynomial fit at the end."
      ],
      "metadata": {
        "id": "yvxHKSNtV7zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = np.arange(1, len(df_train['dG (eV)']))\n",
        "\n",
        "training_mse = []\n",
        "for n in N:\n",
        "  poly_features = sklearn.preprocessing.PolynomialFeatures(degree=n, include_bias=False)\n",
        "  X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "\n",
        "  model = sklearn.linear_model.LinearRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_train, y_pred)\n",
        "  training_mse.append(mse)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(N, training_mse, color='orange')\n",
        "ax.set_xlabel('polynomial order',fontsize=20)\n",
        "ax.set_ylabel('MSE',fontsize=20)\n",
        "ax.set_yscale('log')\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9uUP1R6UWPTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hey, that looks great.  A high-order polynomial gives us a very low MSE.  Let's take a look at what that model looks like."
      ],
      "metadata": {
        "id": "pDSYS8sAXMWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=N[-1], include_bias=False)\n",
        "X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plot_x = np.linspace(np.min(X_train[:,0]), np.max(X_train[:,0]), 100)\n",
        "plot_x_poly = poly_features.transform(plot_x.reshape(-1,1))\n",
        "plot_y = model.predict(plot_x_poly)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train)\n",
        "ax.plot(plot_x, plot_y, color='c', label=str(N[-1])+'th-order polynomial fit')\n",
        "ax.set_ylim(np.min(y_train),np.max(y_train))\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dz3yMcnCXl8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yikes.  Let's check another one."
      ],
      "metadata": {
        "id": "BE960dxhYOuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=N[-20], include_bias=False)\n",
        "X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plot_x = np.linspace(np.min(X_train[:,0]), np.max(X_train[:,0]), 100)\n",
        "plot_x_poly = poly_features.transform(plot_x.reshape(-1,1))\n",
        "plot_y = model.predict(plot_x_poly)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train)\n",
        "ax.plot(plot_x, plot_y, color='c', label=str(N[-20])+'th-order polynomial fit')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9SyM7dP1YOEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surely these models don't reflect reality.  Let's go through our exercise of fitting to many polynomial orders, but this time, we'll also look at the *test* MSE, the MSE evaluated on data that wasn't used to train the model.  We'll load in a new data set for the testing."
      ],
      "metadata": {
        "id": "alWdux65ZGTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('https://raw.githubusercontent.com/albaugh/CHE7507/refs/heads/main/Lecture6/sabatier_test.csv')\n",
        "y_test = df_test['Log(i0)']\n",
        "\n",
        "N = np.arange(1, len(df_train['dG (eV)']))\n",
        "\n",
        "training_mse = []\n",
        "testing_mse = []\n",
        "for n in N:\n",
        "  poly_features = sklearn.preprocessing.PolynomialFeatures(degree=n, include_bias=False)\n",
        "  X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "  X_test = poly_features.transform(df_test[['dG (eV)']].values)\n",
        "\n",
        "  model = sklearn.linear_model.LinearRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_train, y_pred)\n",
        "  training_mse.append(mse)\n",
        "\n",
        "  y_pred_test = model.predict(X_test)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_test, y_pred_test)\n",
        "  testing_mse.append(mse)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(N, training_mse, color='orange', label='training')\n",
        "ax.plot(N, testing_mse, color='purple', label='testing')\n",
        "ax.set_xlabel('polynomial order',fontsize=20)\n",
        "ax.set_ylabel('MSE',fontsize=20)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=16)\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "afnMD7POZqWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that around a polynomial order of 7 the test MSE reaches a minimum.  Around this point we start fitting intrinsic error in the training data.  Let's take a look at the model with the minimum training error."
      ],
      "metadata": {
        "id": "sgNYcXpsag9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argmin(testing_mse)\n",
        "n = N[idx]\n",
        "\n",
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=n, include_bias=False)\n",
        "X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plot_x = np.linspace(np.min(X_train[:,0]), np.max(X_train[:,0]), 100)\n",
        "plot_x_poly = poly_features.transform(plot_x.reshape(-1,1))\n",
        "plot_y = model.predict(plot_x_poly)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train)\n",
        "ax.scatter(df_test['dG (eV)'], df_test['Log(i0)'], color='b', marker='s', label='testing data')\n",
        "ax.plot(plot_x, plot_y, color='c', label=str(n)+'th-order polynomial fit')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8tl6fV1iatJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a much more reasonable model.  And this is a good illustration of the bias-variance tradeoff.  As our model becomes more flexible, its bias will go down- it can fit the data better.  But its variance will increase- it will become more sensitive to changes in the training data.  This is a general principle of machine learning, and we should aim to make our models, as Einstein once said, \"as simple as possible, but no simpler.\"  We should use validation procedures to avoid overfitting.  We'll soon learn about regularization, a way to build models that avoids building in too much variance."
      ],
      "metadata": {
        "id": "4fCUxzqRbksv"
      }
    }
  ]
}