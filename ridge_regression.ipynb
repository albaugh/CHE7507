{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs69SEYqEm6spprFPoHzXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albaugh/CHE7507/blob/main/ridge_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Albaugh.  Wayne State.  CHE 5995/7507.  Lecture 7.  Winter 2026."
      ],
      "metadata": {
        "id": "-aXq53tq2LXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression"
      ],
      "metadata": {
        "id": "vjtkrndl8rKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With polynomial regression we saw that overfitting could be a problem.  Ridge regression offers a way to automatically reduce the magnitude of the fitted parameters.  This helps reduce variance and give more resilient fits.  \n",
        "\n",
        "With standard least squares regression we are minimizing the difference between the model's prediction and the actual values, $||\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y} ||_{2}^{2}$.  With standard least squares regression, the solution for the parameters is $\\boldsymbol{\\beta}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}$.  With ridge regression we add a penalty to our objective function, $||\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y}||_{2}^{2} + \\lambda||\\boldsymbol{\\beta}||_{2}^{2}$ and the solution is $\\boldsymbol{\\beta}=(\\mathbf{X}^{T}\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^{T}\\mathbf{y}$.  Here $\\lambda$ is the ridge parameter, a hyperparameter, that we can change to adjust the strength of the regularization."
      ],
      "metadata": {
        "id": "Ss6ocjz7PHoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAar8UtTPEMA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll demonstate this on the volcano plot data from last time.  Here, $\\log(i_{0})$ is a measure of the speed of the reaction, $\\Delta G$ is the surface adsorption energy, and each data point represents a different metallic catalytic surface."
      ],
      "metadata": {
        "id": "ZMSl6NU0QiSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load training data\n",
        "df_train = pd.read_csv('https://raw.githubusercontent.com/albaugh/CHE7507/refs/heads/main/Lecture6/sabatier_train.csv')\n",
        "x_train = df_train[['dG (eV)']].values\n",
        "y_train = df_train['Log(i0)'].values\n",
        "\n",
        "\n",
        "#load testing data\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/albaugh/CHE7507/refs/heads/main/Lecture6/sabatier_test.csv')\n",
        "x_test = df_test[['dG (eV)']].values\n",
        "y_test = df_test['Log(i0)'].values\n",
        "\n",
        "\n",
        "#visualize the data\n",
        "fig,ax = plt.subplots()\n",
        "def plot_data(ax, df, c, m, l):\n",
        "  ax.scatter(df['dG (eV)'], df['Log(i0)'], color=c, marker=m, label=l)\n",
        "  ax.set_xlabel('$\\\\Delta G$ (eV)',fontsize=20)\n",
        "  ax.set_ylabel('$\\\\log{i_0}$',fontsize=20)\n",
        "\n",
        "plot_data(ax, df_train, 'r', 'x', 'training data')\n",
        "plot_data(ax, df_test, 'b', 'o', 'testing data')\n",
        "ax.grid()\n",
        "ax.legend(fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "27Lhm7mVQi-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that fitting with a high-degree polynomial is a bad idea.  Let's remind ourselves why.  Here's a 20$^{th}$-order polynomial fit to the training data."
      ],
      "metadata": {
        "id": "0DsMnVbQSbNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=20, include_bias=False)\n",
        "X_train = poly_features.fit_transform(x_train)\n",
        "\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plot_x = np.linspace(np.min(X_train[:,0]), np.max(X_train[:,0]), 100)\n",
        "plot_x_poly = poly_features.transform(plot_x.reshape(-1,1))\n",
        "plot_y = model.predict(plot_x_poly)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train, 'r', 'x', 'training data')\n",
        "ax.plot(plot_x, plot_y, color='c', label='20th-order polynomial fit')\n",
        "ax.set_ylim(np.min(y_train), np.max(y_train))\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7jGxFvw2Smrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll apply ridge regularization.  When doing regularization its often a good idea to standardize our data.  This means that for each feature we subtract its mean and then divide by its standard deviation, $x \\to (x - \\mu)/\\sigma$.  This makes all of the features roughly the same size and the fitted coefficients will be roughly the same size as well.  For features across different orders of magnitude or with different units, regularization might affect features unevenly if they are not standardized.  Standardization is a good practice across a range of machine learning techniques."
      ],
      "metadata": {
        "id": "M-yenJGNTkUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#build the polynomial features for the training and test data\n",
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=20, include_bias=False)\n",
        "X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "X_test = poly_features.transform(df_test[['dG (eV)']].values)\n",
        "\n",
        "#here's an example for manually standardizing the data\n",
        "mean = np.mean(X_train, axis=0) #calculate the mean of each feature\n",
        "std = np.std(X_train, axis=0) #calculate the std. dev. of each feature\n",
        "\n",
        "#apply the standardization to the training and test data\n",
        "X_train_standardized = (X_train - mean) / std\n",
        "X_test_standardized = (X_test - mean) / std"
      ],
      "metadata": {
        "id": "LzEJaDJQgGav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<code>sklearn</code> also has its own built-in function for doing this, which can be more convenient."
      ],
      "metadata": {
        "id": "2gtWwx0LiLfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "X_train_standardized = scaler.fit_transform(X_train)\n",
        "X_test_standardized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Z4VhaA8ziTyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <code>alpha</code> keyword in <code>sklearn</code>'s is the $\\lambda$ parameter from our mathematical derivation.  Let's test a range of ridge parameters and caculate the test and training MSE along the way.  We'll also store our fitted model parameters."
      ],
      "metadata": {
        "id": "uhX1pLLQgGHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#choose a range of lambda values across several orders of magnitude\n",
        "lambdas = np.logspace(-4,2,20)\n",
        "\n",
        "training_mse = []\n",
        "testing_mse = []\n",
        "parameters = []\n",
        "for lam in lambdas:\n",
        "  model = sklearn.linear_model.Ridge(alpha=lam)\n",
        "  model.fit(X_train_standardized, y_train)\n",
        "\n",
        "  parameters.append(model.coef_)\n",
        "\n",
        "  y_pred = model.predict(X_train_standardized)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_train, y_pred)\n",
        "  training_mse.append(mse)\n",
        "\n",
        "  y_pred_test = model.predict(X_test_standardized)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_test, y_pred_test)\n",
        "  testing_mse.append(mse)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(lambdas, training_mse, color='orange', marker='o',label='training')\n",
        "ax.plot(lambdas, testing_mse, color='purple', marker='x',label='testing')\n",
        "ax.set_xlabel('$\\\\lambda$',fontsize=20)\n",
        "ax.set_ylabel('MSE',fontsize=20)\n",
        "ax.set_xscale('log')\n",
        "ax.legend(fontsize=16)\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "br6W5qveUHNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training error always gets worse as we regularize because the optimial solution is the non-regularized solution, which we are always penalizing.  However, looking at the test error we can see that as the penalty increases, the error decreases up to a point.  The penalty keeps the model from fitting the intrinsic error.  There is an optimum, after which the penalty becomes too strong and the model cannot fit properly.  Let's take a look at the parameters of our model as a function of $\\lambda$."
      ],
      "metadata": {
        "id": "wcGnttz0kZUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = np.array(parameters)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "colormap = cm.inferno\n",
        "colors = colormap(np.linspace(0.0, 1.0, parameters.shape[1]))\n",
        "\n",
        "for i, c in enumerate(colors):\n",
        "  beta = parameters[:,i]\n",
        "  ax.plot(lambdas, beta, color=c, label='$\\\\beta_{{{}}}$'.format(i+1))\n",
        "\n",
        "ax.set_xlabel('$\\\\lambda$',fontsize=20)\n",
        "ax.set_ylabel('$\\\\beta_{i}$',fontsize=20)\n",
        "ax.set_xscale('log')\n",
        "ax.grid()\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9PB7ByLukdXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the regularization strength increases, the magnitude of the parameters decreases.  Finally, let's take a look at our model with the optimal regularization."
      ],
      "metadata": {
        "id": "iZauoUiSseFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argmin(testing_mse)\n",
        "print(lambdas[idx])\n",
        "\n",
        "model = sklearn.linear_model.Ridge(alpha=lambdas[idx])\n",
        "model.fit(X_train_standardized, y_train)\n",
        "\n",
        "xplot = np.linspace(np.min(df_train[['dG (eV)']].values), np.max(df_train[['dG (eV)']].values), 300)\n",
        "Xplot = poly_features.transform(xplot.reshape(-1,1))\n",
        "Xplot = scaler.transform(Xplot)\n",
        "yplot = model.predict(Xplot)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train, 'r', 'x', 'training data')\n",
        "plot_data(ax, df_test, 'b', 'o', 'testing data')\n",
        "ax.plot(xplot,yplot,color='c',label='20th-order polynomial fit')\n",
        "ax.set_ylim(np.min(y_train), np.max(y_train))\n",
        "ax.set_title('$\\\\lambda=$'+str(np.round(lambdas[idx], decimals=4)),fontsize=20)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aGb8Szyy4Y9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LASSO Regression"
      ],
      "metadata": {
        "id": "uIOBtCRX8wPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at LASSO regression.  With LASSO regression our objective function to minimize is $||\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y}||_{2}^{2} + \\lambda||\\boldsymbol{\\beta}||_{1}^{2}$.  We'll let <code>sklearn</code> numerically find the solution to this, as there is no closed form solution.  As before, we'll examine the test and training MSE for a range of $\\lambda$ values."
      ],
      "metadata": {
        "id": "z4aD16CI8YLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load training data\n",
        "df_train = pd.read_csv('https://raw.githubusercontent.com/albaugh/CHE7507/refs/heads/main/Lecture6/sabatier_train.csv')\n",
        "y_train = df_train['Log(i0)'].values\n",
        "\n",
        "\n",
        "#load testing data\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/albaugh/CHE7507/refs/heads/main/Lecture6/sabatier_test.csv')\n",
        "y_test = df_test['Log(i0)'].values"
      ],
      "metadata": {
        "id": "e9JMmoCS8nbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the polynomial features for the training and test data\n",
        "poly_features = sklearn.preprocessing.PolynomialFeatures(degree=20, include_bias=False)\n",
        "X_train = poly_features.fit_transform(df_train[['dG (eV)']].values)\n",
        "X_test = poly_features.transform(df_test[['dG (eV)']].values)\n",
        "\n",
        "#standardize the data\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "X_train_standardized = scaler.fit_transform(X_train)\n",
        "X_test_standardized = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7UaSHVI69CIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choose a range of lambda values across several orders of magnitude\n",
        "lambdas = np.logspace(-3,-1,15)\n",
        "\n",
        "training_mse = []\n",
        "testing_mse = []\n",
        "parameters = []\n",
        "for lam in lambdas:\n",
        "  model = sklearn.linear_model.Lasso(alpha=lam, max_iter=100000)\n",
        "  model.fit(X_train_standardized, y_train)\n",
        "\n",
        "  parameters.append(model.coef_)\n",
        "\n",
        "  y_pred = model.predict(X_train_standardized)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_train, y_pred)\n",
        "  training_mse.append(mse)\n",
        "\n",
        "  y_pred_test = model.predict(X_test_standardized)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_test, y_pred_test)\n",
        "  testing_mse.append(mse)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(lambdas, training_mse, color='orange', label='training')\n",
        "ax.plot(lambdas, testing_mse, color='purple', label='testing')\n",
        "ax.set_xlabel('$\\\\lambda$',fontsize=20)\n",
        "ax.set_ylabel('MSE',fontsize=20)\n",
        "ax.set_xscale('log')\n",
        "ax.legend(fontsize=16)\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bUihyBX89hHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the parameters change with $\\lambda$."
      ],
      "metadata": {
        "id": "k1vDUTRV--pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = np.array(parameters)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "colormap = cm.inferno\n",
        "colors = colormap(np.linspace(0.0, 1.0, parameters.shape[1]))\n",
        "\n",
        "for i, c in enumerate(colors):\n",
        "  beta = parameters[:,i]\n",
        "  ax.plot(lambdas, beta, color=c, label='$\\\\beta_{{{}}}$'.format(i+1))\n",
        "\n",
        "ax.set_xlabel('$\\\\lambda$',fontsize=20)\n",
        "ax.set_ylabel('$\\\\beta_{i}$',fontsize=20)\n",
        "ax.set_xscale('log')\n",
        "ax.grid()\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9x4iidXq_CCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that with LASSO regression, parameters will get set to 0 exactly.  Previously with ridge regression the parameters would approach 0, but never be 0 exactly.  This is a form of feature selection, where LASSO zeros out the least important features.  Let's look at the optimal model."
      ],
      "metadata": {
        "id": "L86mQMOv_PJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argmin(testing_mse)\n",
        "\n",
        "model = sklearn.linear_model.Lasso(alpha=lambdas[idx], max_iter=100000)\n",
        "model.fit(X_train_standardized, y_train)\n",
        "\n",
        "xplot = np.linspace(np.min(df_train[['dG (eV)']].values), np.max(df_train[['dG (eV)']].values), 300)\n",
        "Xplot = poly_features.transform(xplot.reshape(-1,1))\n",
        "Xplot = scaler.transform(Xplot)\n",
        "yplot = model.predict(Xplot)\n",
        "\n",
        "print('Parameters: ', model.coef_)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "plot_data(ax, df_train, 'r', 'x', 'training data')\n",
        "plot_data(ax, df_test, 'b', 'o', 'testing data')\n",
        "ax.plot(xplot,yplot,color='c',label='20th-order polynomial fit')\n",
        "ax.set_ylim(np.min(y_train), np.max(y_train))\n",
        "ax.set_title('$\\\\lambda=$'+str(np.round(lambdas[idx], decimals=4)),fontsize=20)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bP0AGqsi_ea2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}